{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import en_core_sci_lg\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Users/codyotoole/Desktop/Research_Data/Results/DETM/gd/result_gd.csv'\n",
    "data_df = pd.read_csv(data_path, dtype={\n",
    "    'Date': str,\n",
    "})\n",
    "\n",
    "\n",
    "data_df = data_df.rename(columns={'Content': 'text'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Documents into Paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(0, len(data_df['text'])):\n",
    "    print('splitting by paragraphs...')\n",
    "    \n",
    "    span = 10\n",
    "    docs = []\n",
    "    \n",
    "    splitted_doc = data_df['text'][n].split('. ')\n",
    "    splitted_doc = [\". \".join(splitted_doc[i:i+span]) for i in range(0, len(splitted_doc), span)]\n",
    "    for ii in splitted_doc:\n",
    "        docs.append(ii)\n",
    "    data_df['text'][n] = docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = data_df['text'].apply(pd.Series).reset_index().melt(id_vars='index').dropna()[['index', 'value']].set_index('index')\n",
    "\n",
    "data_df = pd.merge(\n",
    "    pd.merge(\n",
    "        jobs,\n",
    "        data_df['Title'],\n",
    "        left_index=True,\n",
    "        right_index=True),\n",
    "    data_df[['Date']],\n",
    "    left_index=True,\n",
    "    right_index=True).rename(columns={'value': 'text'})\n",
    "\n",
    "data_df.reset_index(drop=True, inplace = True)\n",
    "\n",
    "def get_breaks(content, length):\n",
    "    data = \"\"\n",
    "    words = content.split(' ')\n",
    "    total_chars = 0\n",
    "\n",
    "    # add break every length characters\n",
    "    for i in range(len(words)):\n",
    "        total_chars += len(words[i])\n",
    "        if total_chars > length:\n",
    "            data = data + \"<br>\" + words[i]\n",
    "            total_chars = 0\n",
    "        else:\n",
    "            data = data + \" \" + words[i]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['body_word_count'] = data_df['text'].apply(lambda x: len(x.strip().split()))  # word count in body\n",
    "data_df['body_unique_words']=data_df['text'].apply(lambda x:len(set(str(x).split())))  # number of unique words in body\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop words\n",
    "stopwords = [\n",
    "    'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', \n",
    "    'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', \n",
    "    'al.', 'Elsevier', 'PMC', 'CZI', 'www'\n",
    "]\n",
    "with open('/Users/codyotoole/Desktop/Research_Data/Lemma_Stop/Oxitec_Stop_List.txt', 'r') as f:\n",
    "    stops = f.read().split('\\n')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = string.punctuation\n",
    "parser = en_core_sci_lg.load(disable=[\"tagger\", \"ner\"])\n",
    "#parser.max_length = 7000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(sentence):\n",
    "    mytokens = parser(sentence)\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ]\n",
    "    mytokens = [ word for word in mytokens if word not in stops and word not in punctuations ]\n",
    "    mytokens = \" \".join([i for i in mytokens])\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "data_df[\"processed_text\"] = data_df[\"text\"].progress_apply(spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "\n",
    "for n in range(0, len(data_df)):\n",
    "    w = data_df[\"processed_text\"][n].split(' ')\n",
    "    words.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochLogger(CallbackAny2Vec):\n",
    "    '''Callback to log information about training'''\n",
    "    \n",
    "    def __init__(self):\n",
    "         self.epoch = 0\n",
    "         \n",
    "    def on_epoch_begin(self, model):\n",
    "         print(\"Epoch #{} start\".format(self.epoch))\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "         print(\"Epoch #{} end\".format(self.epoch))\n",
    "         self.epoch += 1\n",
    "         \n",
    "epoch_logger = EpochLogger()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [TaggedDocument(sentence, [i]) for i, sentence in enumerate(words)]\n",
    "    \n",
    "model = Doc2Vec(min_count=2, window=10, size=100, sample=1e-4, negative=5, workers=7)\n",
    "\n",
    "model.build_vocab(sentences)  \n",
    "\n",
    "model.train(sentences, epochs =10, total_examples=model.corpus_count, callbacks=[epoch_logger])\n",
    "\n",
    "model.docvecs[0]\n",
    "model.docvecs.vectors_docs\n",
    "X = np.array(model.docvecs.vectors_docs, dtype='float') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "X_reduced= pca.fit_transform(X)\n",
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run kmeans with many different k\n",
    "distortions = []\n",
    "K = range(2, 20)\n",
    "for k in K:\n",
    "    k_means = KMeans(n_clusters=k, random_state=42).fit(X_reduced)\n",
    "    k_means.fit(X_reduced)\n",
    "    distortions.append(sum(np.min(cdist(X_reduced, k_means.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0])\n",
    "    #print('Found distortion for {} clusters'.format(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_line = [K[0], K[-1]]\n",
    "Y_line = [distortions[0], distortions[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the elbow\n",
    "plt.plot(K, distortions, 'b-')\n",
    "plt.plot(X_line, Y_line, 'r')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 6\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "y_pred = kmeans.fit_predict(X_reduced)\n",
    "data_df['y'] = y_pred\n",
    "\n",
    "#reduce dimensions\n",
    "tsne = TSNE(verbose=1, perplexity=50, random_state=42)\n",
    "X_embedded = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label via k-means\n",
    "# sns settings\n",
    "sns.set(rc={'figure.figsize':(15,15)})\n",
    "\n",
    "# colors\n",
    "palette = sns.hls_palette(6, l=.4, s=.9)\n",
    "sns.set(rc={'axes.facecolor':'white', 'figure.facecolor':'white'})\n",
    "# plot\n",
    "sns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=y_pred, legend='full', palette=palette)\n",
    "plt.title('t-SNE with Kmeans Labels')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Model On The Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizers = []\n",
    "    \n",
    "for ii in range(0, 6):\n",
    "    # Creating a vectorizer\n",
    "    vectorizers.append(CountVectorizer(\n",
    "        min_df=5, max_df=0.9, stop_words='english', lowercase=True, \n",
    "        token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}'))\n",
    "\n",
    "vectorizers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data = []\n",
    "\n",
    "for current_cluster, cvec in enumerate(vectorizers):\n",
    "    try:\n",
    "        vectorized_data.append(cvec.fit_transform(\n",
    "            data_df.loc[data_df['y'] == current_cluster, 'processed_text']))\n",
    "    except Exception as e:\n",
    "        print(\"Not enough instances in cluster: \" + str(current_cluster))\n",
    "        vectorized_data.append(None)\n",
    "\n",
    "len(vectorized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of topics per cluster\n",
    "NUM_TOPICS_PER_CLUSTER = 10\n",
    "\n",
    "lda_models = []\n",
    "for ii in range(0, 10):\n",
    "    # Latent Dirichlet Allocation Model\n",
    "    lda = LatentDirichletAllocation(n_components=NUM_TOPICS_PER_CLUSTER, max_iter=10, learning_method='online',verbose=False, random_state=42)\n",
    "    lda_models.append(lda)\n",
    "    \n",
    "lda_models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_lda_data = []\n",
    "\n",
    "for current_cluster, lda in enumerate(lda_models):\n",
    "    # print(\"Current Cluster: \" + str(current_cluster))\n",
    "    \n",
    "    if vectorized_data[current_cluster] != None:\n",
    "        clusters_lda_data.append((lda.fit_transform(vectorized_data[current_cluster])))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for printing keywords for each topic\n",
    "def selected_topics(model, vectorizer, top_n=5):\n",
    "    current_words = []\n",
    "    keywords = []\n",
    "    \n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        words = [(vectorizer.get_feature_names()[i], topic[i]) for i in topic.argsort()[:-top_n - 1:-1]]\n",
    "        for word in words:\n",
    "            if word[0] not in current_words:\n",
    "                keywords.append(word)\n",
    "                current_words.append(word[0])\n",
    "                \n",
    "    keywords.sort(key = lambda x: x[1])  \n",
    "    keywords.reverse()\n",
    "    return_values = []\n",
    "    for ii in keywords:\n",
    "        return_values.append(ii[0])\n",
    "    return return_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keywords = []\n",
    "for current_vectorizer, lda in enumerate(lda_models):\n",
    "    # print(\"Current Cluster: \" + str(current_vectorizer))\n",
    "\n",
    "    if vectorized_data[current_vectorizer] != None:\n",
    "        all_keywords.append(selected_topics(lda, vectorizers[current_vectorizer]))\n",
    "\n",
    "all_keywords[0]\n",
    "len(all_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's good to save your stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE STUFF\n",
    "f=open('topics.txt','w')\n",
    "\n",
    "count = 0\n",
    "\n",
    "for ii in all_keywords:\n",
    "\n",
    "    if vectorized_data[count] != None:\n",
    "        f.write(', '.join(ii) + \"\\n\")\n",
    "    else:\n",
    "        f.write(\"Not enough instances to be determined. \\n\")\n",
    "        f.write(', '.join(ii) + \"\\n\")\n",
    "    count += 1\n",
    "\n",
    "f.close()\n",
    "\n",
    "# save the final t-SNE\n",
    "pickle.dump(X_embedded, open(\"X_embedded.p\", \"wb\" ))\n",
    "\n",
    "# save the labels generate with k-means(20)\n",
    "pickle.dump(y_pred, open(\"y_pred.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue Onward\n",
    "Classify and test your k-means clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report(model_name, test, pred):\n",
    "    from sklearn.metrics import precision_score, recall_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import f1_score\n",
    "    \n",
    "    print(model_name, \":\\n\")\n",
    "    print(\"Accuracy Score: \", '{:,.3f}'.format(float(accuracy_score(test, pred)) * 100), \"%\")\n",
    "    print(\"     Precision: \", '{:,.3f}'.format(float(precision_score(test, pred, average='macro')) * 100), \"%\")\n",
    "    print(\"        Recall: \", '{:,.3f}'.format(float(recall_score(test, pred, average='macro')) * 100), \"%\")\n",
    "    print(\"      F1 score: \", '{:,.3f}'.format(float(f1_score(test, pred, average='macro')) * 100), \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set size of 20% of the data and the random seed 42 <3\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.toarray(),y_pred, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"X_train size:\", len(X_train))\n",
    "print(\"X_test size:\", len(X_test), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD instance\n",
    "sgd_clf = SGDClassifier(max_iter=10000, tol=1e-3, random_state=42, n_jobs=4)\n",
    "# train SGD\n",
    "sgd_clf.fit(X_train, y_train)\n",
    "\n",
    "# cross validation predictions\n",
    "sgd_pred = cross_val_predict(sgd_clf, X_train, y_train, cv=3, n_jobs=4)\n",
    "\n",
    "# print out the classification report\n",
    "classification_report(\"Stochastic Gradient Descent Report (Training Set)\", y_train, sgd_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation predictions\n",
    "sgd_pred = cross_val_predict(sgd_clf, X_test, y_test, cv=3, n_jobs=4)\n",
    "\n",
    "# print out the classification report\n",
    "classification_report(\"Stochastic Gradient Descent Report (Training Set)\", y_test, sgd_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_cv_score = cross_val_score(sgd_clf, X.toarray(), y_pred, cv=10)\n",
    "print(\"Mean cv Score - SGD: {:,.3f}\".format(float(sgd_cv_score.mean()) * 100), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_path = 'topics.txt'\n",
    "with open(topic_path) as f:\n",
    "    topics = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns settings\n",
    "sns.set(rc={'figure.figsize':(15,15)})\n",
    "\n",
    "# colors\n",
    "palette = sns.hls_palette(6, l=.4, s=.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'axes.facecolor':'white', 'figure.facecolor':'white'})\n",
    "# plot\n",
    "sns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=y_pred, legend='full', palette=palette)\n",
    "\n",
    "plt.title('t-SNE with Kmeans Labels')\n",
    "\n",
    "plt.text(-60, 20, 'Disease, Virus, Transmission', fontweight='bold', fontsize=15,\n",
    "        bbox={'facecolor': palette[0], 'alpha': 0.8, 'pad': 10})\n",
    "plt.text(-15, -20, 'CRISPR, Genetic Engineering', fontweight='bold', fontsize=15,\n",
    "        bbox={'facecolor': palette[1], 'alpha': 0.8, 'pad': 10})\n",
    "plt.text(-40, 35, 'Risk, Release, Public', fontweight='bold', fontsize=15,\n",
    "        bbox={'facecolor': palette[2], 'alpha': 0.8, 'pad': 10})\n",
    "plt.text(-37, 0, 'Pests/Insects, Vector-Borne', fontweight='bold', fontsize=15,\n",
    "        bbox={'facecolor': palette[3], 'alpha': 0.8, 'pad': 10})\n",
    "plt.text(-20, -67, 'Expression, Mutation, Genes', fontweight='bold', fontsize=15,\n",
    "        bbox={'facecolor': palette[4], 'alpha': 0.5, 'pad': 10})\n",
    "plt.text(20, 10, 'Drive, Edit, Transgenic', fontweight='bold', fontsize=15,\n",
    "        bbox={'facecolor': palette[5], 'alpha': 0.8, 'pad': 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(\"final_cluster_tsne.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
